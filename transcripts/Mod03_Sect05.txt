 Hi, welcome back to Module 3. This is Section 5 on training. In this section, we're going to look at how to select a model and train it with the data we have pre-processed. At this point, you've done a lot to clean and prepare your data, but that doesn't mean your data is completely ready to train the algorithm. Some algorithms may not be able to work with training data in a data frame format. Some file formats like CSV are commonly used by various algorithms, but they do not make use of that optimization that some of the file formats like Record.io ProtoBuff can use. Many Amazon SageMaker algorithms support training with data in a CSV format. Amazon SageMaker requires that a CSV file doesn't have a header record and that the target variable is in the first column. Most Amazon SageMaker algorithms work best when you use the optimized ProtoBuff Record I.O. format for the training data. Using this format allows you to take advantage of pipe mode when training the algorithms that support it. In pipe mode, your training job streams data directly from Amazon S3. When using the CSV format, the target variable in your training data set should be the first column on the left and your features should be to the right of the target variable column. Evaluating a model with the same data that it trained on will lead to overfitting. Recall overfitting is where your model learns the particulars of a data set too well. It's essentially memorizing the training data rather than learning the relationships between features and labels. This means the model isn't learning from those relationships and patterns to apply them to new data in the future. Holdout is when you split your data into multiple sets, commonly sets for training data, validation data, and testing data. Training data, which includes both features and labels, feeds into the algorithm you've selected to produce your model. You then use the model to make predictions over the validation data set, which is where you'll likely notice things you'll want to tweak, and tune, and change. Then when you're ready, you run the test data set, which only includes features, since you want the labels to actually be predicted. The performance you get here with the test data set is what you can reasonably expect to see in production. A common split when using the holdout method is using 80% of the data for a training set, 10% for validation, and 10% for test. Or if you have a lot of data, you can split it into 70% training, 15% validation, and 15% test. So for a small data set, we can use k-fold cross-validation to utilize as much of the data as possible, while still having relatively good metrics in order to choose which model is better. K-fold cross-validation randomly partitions the data into k different segments. For each segment, we'll use the rest of the data outside of it for training in order to do a validation on that particular segment. Let's look at an example. Here we have a five-fold cross-validation. The available training data is separated into five different chunks. For the training of the first model, we're using all those chunks as the training data, and then we're going to calculate the metrics on this test piece. For the second model, we're going to use these pieces as training. Even if the model is trained, you apply it to this test piece. We do the same thing five times. We use all the training data, and we test it on five different models on different chunks of the test data, eventually testing it on all data points. One other thing to note about splitting your data, data in a specific order can lead to biases on your model. This is especially true if you're working with structured data. For example, the wine data is ordered by the quality column. When you run your model against your test data, this ordered pattern will be applied, biasing the model. It might also mean that some targets are missing from the training data. Typically, randomizing your dataset prior to splitting is sufficient, and many libraries will provide functions for this. With smaller sets, it is sometimes useful to use stratified sampling. Stratified sampling ensures that the training and test sets have approximately the same percentage of samples of each target class as the complete set. An internet search will give you many ways to shuffle and split the data. One of the easiest is to use the train test split function from SKlearn. Amazon SageMaker provides four different ways you can train models. The built-in algorithms available can be easily deployed from the AWS console, CLI, or a Jupyter notebook. Containers are used behind the scenes when you use one of the Amazon SageMaker built-in algorithms, but you do not have to deal with them directly. Amazon SageMaker supported frameworks provide pre-built containers to support deep learning frameworks such as Apache MXNet, TensorFlow, PyTorch, and Chainr. It also supports machine learning libraries such as Skykit Learn and SparkML by providing pre-built Docker images. If you use the Amazon SageMaker Python SDK, they are deployed using their respective Amazon SageMaker SDK estimator class. If there is no pre-built Amazon SageMaker container image that you can use or modify for an advanced scenario, you can package your own script or algorithm to use with Amazon SageMaker. You can use any programming language or framework to develop your container. For an example, if your team works and builds ML models in R, you can build your own containers to train and host an algorithm in R as well. Someone else may have already developed and tuned a model. It is worth looking in the AWS Marketplace to find available models. Amazon SageMaker provides high-performance, scalable machine learning algorithms optimized for speed, scale, and accuracy. For supervised learning, Amazon SageMaker includes XGBoost and Linear Learner algorithms for classification and quantitative or regression problems. There is also a factorization machine to address recommendation and time series prediction problems. Amazon SageMaker includes support for unsupervised learning, such as with K-means clustering, and principal component analysis, PCA, to solve problems like identifying customer groupings based on purchasing behavior. Finally, there are a selection of specialized algorithms for processing images and other deep learning tasks. Let's look a little closer at three of the most commonly used built-in algorithms and their use cases. XGBoost, or Extreme Gradient Boosting, is a popular and efficient open-source implementation of the Gradient Boosted Trees algorithm. Gradient Boosting is a supervised learning algorithm that attempts to accurately predict a target variable by combining an ensemble of estimates from a set of simpler, weaker models. XGBoost has done remarkably well in machine learning competitions because it robustly handles a variety of data types, relationships, and distributions. The large number of hyperparameters can be tweaked and tuned for improved fit. This flexibility makes XGBoost a solid choice for problems in regression, classification, binary and multi-class, and ranking. The Amazon SageMaker Linear Learner algorithm provides a solution for both classification and regression problems. With the Amazon SageMaker algorithm, you can simultaneously explore different training objectives and choose the best solution from your validation set. You can also explore a large number of models and choose the best one for your needs. Compared with methods that provide a solution for only continuous objectives, the Amazon SageMaker Linear Learner algorithm provides a significant increase in speed over naive hyperparameter optimization techniques. K-Means is an unsupervised learning algorithm. It attempts to find discrete groupings within data where members of a group are as similar as possible to one another and as different as possible from members of other groups. You define the attributes that you want the algorithm to use to determine similarity. To train a model in Amazon SageMaker, you create a training job. The training job includes the URL of the Amazon S3 bucket where you stored the training data. The URL of the S3 bucket where you want to store the output of the job. The Amazon Elastic Container Registry path where the training code is stored. The compute resources that you want Amazon SageMaker to use for model training. Compute resources are ML compute instances managed by Amazon SageMaker. Amazon SageMaker provides a selection of instance types optimized to fit different machine learning use cases. Instance types comprise varying combinations of CPU, GPU, memory and networking capacity and give you the flexibility to choose the appropriate mix of resources for building, training and deploying your ML models. Each instance type includes one or more instance sizes allowing you to scale your resources to the requirements of your target workload. Some key takeaways from this section of the module include split data into training and testing sets helps you validate the model's accuracy. K-fold cross validation can help with smaller data sets. Two key algorithms for supervised learning are XGBoost and Linear Learner. Use K-means for unsupervised learning. And use Amazon SageMaker to train models. That's it for section 5. I hope to see you in the next video.