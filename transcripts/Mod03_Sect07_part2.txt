 Hi, welcome back. We'll continue exploring how to evaluate your model. The diagram shows the confusion matrix of how two different models performed on the same data. Can you tell which one's better? Which is better isn't a good question to ask. What do you mean by better? Does better mean making sure you find all the cats? Even if it means you'll get many false positives? Or does better mean making sure the model is the most accurate? It's difficult to see just by looking at the two charts. What if you're trying several models using multiple folds and have hundreds of data points to compare? To do that, you'll need to calculate more metrics. The first metric is sensitivity. This is sometimes referred to as recall, hit rate, or true positive rate. Sensitivity is the percentage of positive identifications. In the cat example, it represents what percentage of cats were correctly identified. To calculate sensitivity, take the number of true positives, or the number of positive identifications of cats, and divide that by the total number of actual cats. In this example, 60% of cats that were cats were correctly identified as cats. Specificity is sometimes referred to as selectivity or true negative rate. Specificity is the percentage of negatives correctly identified. In the cat example, this is the number of images that were not cats that were correctly identified as not cats. To calculate specificity, take the number of true negatives and divide that by the total number of actual negatives. So for the example, that's the number of not cats that were correctly identified divided by the total number of actual not cats. This means that in the example, 64% of not cats were identified as not cats. Now that you have these metrics for each model, knowing what your business goal is makes it easier to decide which model to use. Which model would you choose if you wanted to make sure you'll identify as many cats as possible? Model B would be a good answer if you're not concerned about having many false positives that is. If you're not concerned about having incorrectly identified not cats. Which model would you choose if you wanted to make sure you identified animals that were not cats? Model A might work for this scenario. Again, it would depend on how many false negatives you can tolerate. If this was a classification of patients who had heart disease or not, which model would be best? This is where it gets interesting. A fun website might get a bad reputation if it can't identify cats correctly, but if you're trying to diagnose patients, your focus will probably be very different. It's important to understand the trade-offs you're making when you decide which model to use. There are also other metrics that can help you make your decisions. That's it for part two of this section. We'll see you again for part three where we'll start looking at thresholds.