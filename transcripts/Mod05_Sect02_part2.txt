 Hi, welcome back. We'll continue exploring image analysis with a closer look at facial detection. Facial detection uses a model that was tuned to perform predictions specifically for detecting faces and facial features. Facial detection has many of the same features as standard object detection, such as a bounding box, or the coordinates of the box surrounding the face that was detected. This will include a value representing the confidence that the bounding box contains a face. There will be a list of attributes, if found, such as if the face has a beard, or if it appears to be male or female. There will also be a confidence score for these attributes. It can also detect physical emotions, like whether the person is smiling or frowning. It's important to understand this classification is based only on visual clues, and so it might not represent the actual emotion of the person. Facial landmarks are components of the face, such as eyes and mouth. Typical landmarks also include X and Y coordinates. Faciality describes the brightness and the sharpness of the face, and pose describes the rotation of the face inside the image. Again, confidence is a feature here, and it's provided for each detected feature. And remember, the feature prediction is based only on visual observations. With Amazon Recognition, you can compare two images to determine if they contain the same person. Comparisons require both a source and a target image. The results will include all the faces that were found, and they include information about matching and non-matching faces. Again, confidence scores indicate how likely each prediction is. Amazon Recognition can also search for known faces. To use this feature, you need to train the model by providing a collection of images to use. After you train the model, you can then detect those people and images you provide. To find known faces, first, create a collection and add faces to the collection. Amazon Recognition will perform facial recognition on the images you provide. It will then return typical information, like the bounding box coordinates or the confidence score. To associate faces with an image, specify an image ID in the external image ID request parameter. This could be the file name of the image or another ID that you create. After you create your collection, you can then use the search faces by image operation to search for faces from the collection. The returned data contains an array of all faces that matched. The information includes bounding boxes, confidence scores, and the external image ID value. You can then use this ID value to link back to the source image. Now that you've learned about the facial detection features of Amazon Recognition, here's a summary of the guidelines we've discussed so far. When Amazon Recognition detects a human face, it captures a bounding box that shows where the face was found in the video. It can also detect attributes such as the position of the eyes, nose, and mouth. It can detect emotion, the quality of the detection, and any landmarks that might appear. All these items will have an associated confidence score. A higher score means that the model has greater confidence about the detection. Gender is inferred from the image, not inferred from identity. Similarly, emotion is also determined from the image, and it might not reflect the subject's actual emotional state. How should I apply facial recognition responsibly? Facial recognition should never be used in a way that violates an individual's rights, including the right to privacy. It should also never be used to make autonomous decisions for scenarios that require a human being to analyze them. For example, suppose that a bank uses tools like Amazon Recognition in a financial application to verify their customer's identities. The bank should always clearly disclose the use of the technology and ask the customer to approve their terms and conditions. For more information about this topic, see the AWS web page about the facts on facial recognition with artificial intelligence. We'll now explain how you can use Amazon Recognition to process videos. You can perform video processing on both stored videos and video streams. Stored videos should be uploaded and stored in an S3 bucket. Each type of detection has its own start operation. You can search for people, faces, labels, celebrities, text, and inappropriate content. Amazon Recognition publishes a completion status to a topic in Amazon Simple Notification Service, which is also known as Amazon SNS. Then, SNS can route these messages to subscribers. For durability, it's a best practice to route messages to a message queue in Amazon Simple Queue Service or Amazon SQS. Your application should monitor the SQS queue for completion. Each start operation has a corresponding GET operation for retrieving the results. If you call GETDetectionResults, it returns an array of labels that contain information about any labels found in the video. The label information includes the same labels as image detection, but it also includes a timestamp of where the label was detected in milliseconds from the start of the video. In addition to stored videos, you can also use Amazon Recognition video to detect and recognize faces in streaming video. A typical use case for this is detecting a known face in a video stream. Amazon Recognition video uses Amazon Kinesis video streams to receive and process a video stream. The analysis results are output from Amazon Recognition video to a Kinesis data stream. They are then read by your client application. Amazon Recognition video provides a stream processor that's called Create Stream Processor, and you can use it to start and manage the analysis of the streaming video. To use Amazon Recognition video with your streaming video, your application must implement these resources. First, you need a Kinesis video stream to send streaming video to Amazon Recognition video. Next, you need an Amazon Recognition video stream processor to manage the streaming video analysis. And finally, you need a Kinesis data stream consumer to read the analysis results that Amazon Recognition video sends to the data stream. If you want to find a face in a video, you need to create a collection. This process is the same as creating a collection for still images. Amazon Recognition video places a JSON frame record for each analyzed frame into the Kinesis output stream. Amazon Recognition video doesn't analyze every frame that's passed to it through the Kinesis video stream. A frame record that's sent to a Kinesis data stream contains information about which video stream fragment the frame is in, where the frame is in the fragment, and faces that are recognized in the frame. It also includes status information for the stream processor. Before we wrap up, here's a quick summary. Amazon Recognition is a computer vision service that's based on deep learning. You can easily add image and video analysis to your applications. Amazon Recognition can detect faces, sentiment, text, unsafe content, and library search in both images and video. Amazon Recognition is integrated with other AWS services. Thanks for watching. We'll see you in the next video.