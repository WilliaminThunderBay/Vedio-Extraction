 Hi, welcome back. We'll continue exploring how to describe your data. Now that your data is in a readable format, you can perform descriptive statistics on the data to better understand it. Descriptive statistics help you gain valuable insights into your data so that you can effectively pre-process the data and prepare it for your ML model. We'll look at how you can do that and discuss why it's so important. First, descriptive statistics can be organized into a few different categories. Overall statistics include the number of rows and the number of columns in your data set. This information, which relates to the dimensions of your data, is very important. For example, it can indicate that you have too many features, which can lead to high dimensionality and poor model performance. Attribute statistics are another type of descriptive statistic, specifically for numeric attributes. They're used to get a better sense of the shape of your attributes. This includes properties like the mean, standard deviation, variance, and minimum and maximum values. If you need to look at relationships between more than one variable, you can consider multivariate statistics. They mostly relate to the correlations and relationships between your attributes. For cases when you have multiple variables or features, you might want to look at the correlations between them. It's important to identify correlations between attributes because a high correlation between two attributes can sometimes lead to poor model performance. When features are closely correlated and they're all used in the same model to predict the response variable, there could be problems. For example, the model loss might not converge to a minimum state. So be aware of highly correlated features in your data set. Mean and median are two different measures describing the extent that your data is clustered around some value or position. Mean can be a useful method for understanding your data when the data is symmetrical. However, if your data is skewed or contains outliers, then median tends to provide the better metric for understanding your data as it relates to central tenancy. For instance, if you have outliers with large values, the mean can be skewed one way and it wouldn't serve as an accurate representation of where your values are truly centered. Median isn't affected by outliers in the same way. We'll talk more about outliers soon. Statistics are available and they can be viewed on numerical data by using methods such as describe. There are also other methods to calculate the mean, median, and others. You can also view statistics on single or multiple columns. You can even group data by specific values. For categorical attributes, you can look at the frequency of attribute values in your data set. That information will give you some idea about what is inside that categorical variable. The diagram here shows the CAR data set, which is made up of several categorical values, buying, mate, lug boot, safety, and class. Safety can be either low, medium, or high. From the describe function, you can see that there are three unique values, with low being the most frequent. Looking at the class column, it appears that the top value of the four is UNACC, which stands for unaccounted. This accounts for 1,210 of the 1,728 values, or 70%. This might suggest an imbalance. For a target variable that's also of a categorical type, you can look at the class distribution to see whether there's a class imbalance in your data set. Inbalance data can mark a disproportionate ratio for your classes. For instance, your data set is made up of credit card transactions, but only a tenth of a percent is labeled as fraud. In this case, your algorithm might not learn well enough to predict examples of credit card fraud. Visualization could help you gain insights into your data that you might not be aware of otherwise. A histogram is often a good visualization technique for seeing the overall behavior of a particular feature. With a histogram, you can answer questions like, is the feature data normally distributed? How many peaks are there in the data? Is there any skewness for that particular feature? When using histograms for your data visualization, values are binned. The taller peaks of the histogram indicate the most common values. For numerical features, you can use density plots and box plots, in addition to histograms, to get an idea of what's inside that particular feature. Like a histogram, these visualizations will help you answer questions like, what's the range of the data? The peak of the data? Are there any outliers? Are there any special features? Using these questions helps you understand your data better and can also help you decide if you need to do more specialized data pre-processing. A box plot is a method for graphically depicting groups of numerical data through their quartiles. When you have more than two numerical variables in a feature dataset, you might want to look at their relationship. A scatter plot is a good way to identify any special relationships among those variables. In this case, the left diagram has sulfates and alcohol. They are two numerical variables. Suppose you want to show the relationship between these variables. You can use a scatter plot to help you visualize that. There are plots scattered around, and the correlation among them might not be that high because the data is scattered. However, you might find some relatively positive relationships between the two variables. Scatter plot matrices help you look at the relationship between multiple different features. In PENDIS, you can easily create scatter plot matrices based on the columns you want to look at. This example has three columns, and it will give the pairwise scatter plot for any two columns. With a scatter plot, you might want to identify special regions that a particular subset of data could fit into. In the example, is there a relationship between alcohol sulfates and quality? You could plot those values against good and poor quality wines like the example. Plotting gives you an idea of how useful particular variables can be if you're using them for a classification problem. That's it for part two of this section. We'll see you again for part three, where we'll review correlations and the takeaways for this section.