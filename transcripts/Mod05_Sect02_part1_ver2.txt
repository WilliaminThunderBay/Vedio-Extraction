 Welcome back. In this section, we'll explore image analysis in more detail. And in part two, we'll take a closer look into video analysis. To start, we'll introduce the main Amazon service we'll be using, Amazon Recognition. Amazon Recognition is a computer vision service that's based on deep learning. You can use it to add image and video analysis to your applications. There are many uses for Amazon Recognition, including creating searchable image in video libraries. Amazon Recognition makes both images and stored videos searchable, so you can discover the objects and scenes that appear in them. You can use Amazon Recognition to build a face-based user verification system, so your applications can confirm user identities by comparing their live image with a reference image. Amazon Recognition interprets emotional expressions such as happy, sad, or surprise. It can also interpret demographic information from facial images, such as gender. Amazon Recognition can also detect inappropriate content in both images and stored videos. And finally, Amazon Recognition can recognize and extract text content from images. Before we go further, here's a quick note on security. You need to check if the applications you build using Amazon Recognition fall under any regulatory restrictions as defined in your field or country. Security and compliance for Amazon Recognition is a shared responsibility between AWS and the customer. For more information about this topic, see the AWS Compliance page. Amazon Recognition is an AWS Managed Service. With a managed service, Amazon hosts the machine learning models, maintains an API, and scales out to meet demand for you. You can benefit from a set of models that constantly learn and improve. Also, you can focus on building applications that use the API and optionally training the service to understand your unique business needs. There are various resources you can use to access and interact with Amazon Recognition, such as APIs, SDKs, and commands for the AWS command line interface, which is also known as the AWS CLI. The languages supported by the SDKs include JavaScript, Python, PHP, .NET, Ruby, Java, Go, Node.js, and C++. Finally, Amazon Recognition integrates with other AWS services. For example, if you need storage, you can use Amazon's simple storage service, or S3. For authentication and authorization, you can use AWS Identity and Access Management, which is also known as IAM. This diagram illustrates an image search feature where users can take pictures and get information about the real estate properties they're viewing. First, the user takes a picture with their mobile device. The user then initiates a search, which causes the application to upload to Amazon S3. S3 is configured to call other services when a write event occurs. In this case, the bucket passes the S3 path of the new object to AWS Lambda. When the Lambda function is called, it uses the Amazon Recognition SDK to call the service. Amazon Recognition analyzes the image, detects aspects of the property, creates labels, and passes the information back to Lambda as an object formatted in JavaScript Object Notation or JSON. Lambda then stores the labels and confidence score in Amazon Elastic Search Service, which is also known as Amazon ES. Application users can now identify aspects of a property using the objects that were detected in the image. In this example architecture, the system checks uploaded images for inappropriate content. Like the previous example, processing begins when the user uploads content. First, the user uploads an image to Amazon S3. Second, the S3 bucket is configured to call a Lambda function when an object is written to the bucket. Third, Lambda calls Amazon Recognition via the SDK. Amazon Recognition then analyzes the images for inappropriate content and sends the response back to Lambda. Fourth, if the content is appropriate, the content is approved. Fifth, if the content isn't appropriate, the content can be sent for manual inspection. And finally, if the content isn't approved, a notification is sent to the user. In this final use case, the system analyzes a video feed for sentiment analysis. First, an in-store camera captures video that's then sent to a back office or a cloud-based application. Typically, an application like this uses Amazon Kinesis to stream the video. Second, the application uses the SDK to send the video to Amazon Recognition for further analysis. Visual sentiment is extracted along with other attributes such as age. Third, the discovered attributes are sent to Amazon Kinesis. Fourth, a Lambda function extracts the data from the stream. Fifth, the data is then written to S3. Next, the data is loaded into Amazon Redshift on a regular basis. And finally, tools like Amazon QuickSight can be used to generate reports from the data. Amazon Recognition is designed to integrate into your applications through the API and SDKs. API operations are provided for detecting labels, faces, recognizing celebrities, and detecting unsafe images. To perform a prediction, provide the service with an image object in Amazon S3 or upload a byte stream of an image. Images can be in JPEG or PNG formats. Amazon Recognition processes the image, performs the prediction, and returns a JSON object with the results. When Amazon Recognition performs predictions, it often returns multiple labels. Each label has a confidence level. This confidence level indicates how likely the label was found in the image. Like this example shows, labels can also have hierarchies. When you find instances of objects, you need to understand where the detected object is in the image. For each instance, the results from Amazon Recognition include a bounding box that contains the starting coordinate of top, left, and box dimensions of width, height. Like the example, you can use this information to determine the location of the detected object in the image. It's important to note that all findings contain a confidence score. You can use the confidence score in your applications to tune your response to predictions. With a higher score, it's more likely that the object was correctly labeled. That's it for part one of this section. We'll see you again for part two, where we'll explore facial detection.