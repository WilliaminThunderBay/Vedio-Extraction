 Hi, welcome back. We'll continue exploring video analysis by reviewing how to create the test dataset. The final step before you train your model is to identify a test dataset. You will use this test dataset to validate and evaluate the model's performance. You'll do this by performing an inference on the images in the test dataset. You'll then compare the results with the labeling information that's in the training dataset. You can create your own test dataset. Alternatively, you can use Amazon Recognition Custom Labels to split your training dataset into two datasets by using an 80-20 split. This split means that 80% of the data is used for training and 20% is used for testing. After you define the training and test datasets, Amazon Recognition Custom Labels can automatically train the model for you. The service automatically loads and inspects the data, selects the correct machine learning algorithms, trains a model, and provides model performance metrics. You're charged for the amount of time a model takes to train. A dataset that contains more images and labels will take longer to train. When training is complete, you evaluate the performance of the model. During testing, Amazon Recognition Custom Labels predicts if a test image contains a custom label. The confidence score is a value that quantifies the certainty of the model's prediction. Because this is a classification problem, the results can be mapped to a confusion matrix. With a true positive, the model correctly predicts the presence of the custom label in the test image. That is, the predicted label is also a ground truth label for that image. For example, Amazon Recognition Custom Labels correctly returns a cat label when a cat is present in an image. For a false positive, the model incorrectly predicts the presence of a custom label in a test image. That is, the predicted label isn't a ground truth label for the image. For example, Amazon Recognition Custom Labels returns a cat label, but there's no cat label in the ground truth for that image. For a false negative, the model doesn't predict that a custom label is present in the image, but the ground truth for that image includes this label. For example, Amazon Recognition Custom Labels doesn't return a cat custom label for an image that contains a cat. With a true negative, the model correctly predicts that a custom label isn't present in the test image. For example, Amazon Recognition Custom Labels doesn't return a cat label for an image that doesn't contain a cat. The console provides access to true positive, false positive, and false negative values for each image in your test data set. These prediction results are used to calculate the various metrics for each label and an aggregate of metrics for your entire test set. The same definitions apply to predictions that the model makes at the bounding box level. With bounding boxes, all metrics are calculated over each bounding box in each test image, regardless of whether the boxes are prediction or ground truth. To help you, Amazon Recognition Custom Labels provides various metrics. For example, you can view summary metrics and evaluation metrics for each label. It also provides precision metrics for each label and an average precision metric for the entire test data set. Precision is the proportion of positive results that were correctly classified. Amazon Recognition Custom Labels provides average recall metrics for each label and an average recall metric for the entire test data set. Recall is the fraction of your test set labels that were correctly classified. Using the previous example of cats, that would be how many cats were correctly classified. The service also provides an average model performance score for each label and an average model performance score for the entire test data set. The F1 score combines precision and recall together to give you just one number that quantifies the overall performance of a particular machine learning algorithm. You might use the F1 score when you have a class imbalance, but you also want to preserve the equality between precision and sensitivity. A higher value means better model performance for both recall and precision. If you're satisfied with the accuracy of your model, you can start using it. That's it for part three of this section. We'll see you again for part four, where we'll review how to evaluate and improve your model.