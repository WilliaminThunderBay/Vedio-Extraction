 Hi, welcome back. We'll continue exploring data collection by reviewing how to extract, transform, and load data. Data is typically spread across many different systems and data providers. This presents a challenge. You'll need to bring all these data sources together into something that can be consumed by a machine learning model. You can do this through extract, transform, and load, which is also known as ETL. The steps in ETL are defined this way. In the extract step, you pull the data from the sources to a single location. During extraction, you might need to modify the data, combine matching records, or do other tasks that transform the data. Finally, in the load step, the data is loaded into a repository, such as Amazon S3. A typical ETL framework has several components. As an example, consider the diagram. First, the Crawler A program connects to a data store, which can be a source or a target. It progresses through a ranked list of classifiers to determine the schema for your data. Then, it creates metadata tables in the AWS Glue Data Catalog. A job defines the business logic that's needed to perform ETL work. To run the job, you'll need to use a schedule or event. As a final note, the services we just discussed exist in the transform partition of the ETL process. AWS Glue is a fully managed ETL service that makes it simple and cost effective to categorize your data, clean it, enrich it, and move it reliably between various data stores. AWS Glue consists of a central metadata repository known as the AWS Glue Data Catalog. This is an ETL engine that automatically generates Python or Scala code. It also provides a flexible scheduler that handles dependency resolution, job monitoring, and retries. AWS Glue is serverless, so you don't need to set up or manage any infrastructure. You can use the AWS Glue Console to discover data, transform it, and make it available for search inquiries. The console calls the underlying services to orchestrate the work needed to transform your data. You can also use the AWS Glue API operations to interface with the AWS Glue services. This way, you can edit, debug, and test your Python or Scala Apache Spark ETL code using a familiar development environment. AWS Glue is well suited to machine learning because it can receive labeled data that can be used for training. Here's an example. Say that you provide AWS Glue with training data that teaches the model what duplicate records in the data source look like. Then AWS Glue can identify the duplicates and present them for further analysis by a data engineer. AWS Glue enables the orchestration of complex ETL jobs. In the example, AWS Glue crawls the data sources and presents the information to clients as a data catalog. AWS Glue can run your ETL jobs based on an event, such as getting a new data set. For example, you can use an AWS Lambda function to trigger your ETL jobs to run as soon as new data becomes available in Amazon S3. You can also register this new data set in the AWS Glue data catalog as part of your ETL jobs. Although managed tools are available in AWS to manipulate data, a data scientist will also write scripts in their Jupyter Notebook to handle data. A very simple extract and load script is shown here. The Imports and Variables section imports the libraries that are used. Note that BOTO3 is the library for AWS. Variables are also set here for the zip files web location and a local folder for extraction. The Download and Extract section makes a web request, saving the bytes from the URL as a stream. This stream is passed to the zip file function, which is then used to extract the data. With the extracted files in a folder, the Upload to S3 section enumerates the folder's files and uploads each file to Amazon S3. If you discover that this script is used often, it should be migrated to a standalone function that can be imported by other Python applications. That's it for part two of this section. We'll see you again for part three, where we'll review how to secure your data.