 We'll get started by reviewing what natural language processing means. Natural language processing is also known as NLP. Before we explain what NLP is, we'll consider an example of NLP, Amazon Alexa. Alexa works by having a device, such as an Amazon Echo, record your words. The recording of your speech is sent to Amazon servers to be analyzed more efficiently. Amazon breaks down your phrase into individual sounds. Then, it connects to a database containing the pronunciation of various words to find which words most closely correspond to the combination of individual sounds. Amazon identifies important words to make sense of the tasks and carry out corresponding functions. For instance, if Alexa notices words like outside or temperature, it will open the weather, Alexa skill. Amazon servers then send the information back to your device and Alexa speaks. NLP is a broad term for a general set of business or computational problems you can solve with machine learning or ML. However, NLP systems pre-date machine learning. For example, speech-to-text on older pre-smartphone cell phones used NLP and so did screen readers. Many NLP systems now use some form of machine learning. NLP considers the hierarchical structure of language. Words are at the lowest layer in a hierarchy. A group of words make a phrase. In the next level up, phrases make a sentence. And ultimately, sentences convey ideas. NLP systems face several significant challenges. We'll look at its challenges next. Language isn't precise. Words can have different meanings based on the other words that surround them. This is known as context. Often the same words or phrases can have multiple meanings. For example, consider the term weather. You could be under the weather, which has a colloquial meaning in English that you're sick. Or you could say there's wonderful weather outside, which means the weather conditions outside are good. The phrase, oh really, could convey surprise, disagreement, and many other things. It depends on the context and inflection. Here are some of the main challenges for NLP. One challenge is discovering the structure of the text. One of the first tasks of any NLP application is to break down the text into meaningful units, such as words, phrases, and sentences. Another challenge is labeling data. After the system converts the text to data, it must apply labels representing the various parts of speech. Every language will require a different labeling scheme to match the language's grammar. NLP also faces a challenge in representing context. Because word meaning can depend heavily on context, any NLP system needs a way to represent it. This is a large challenge because there are many contexts, and it's difficult to convert context into a form computers can understand. Finally, although grammar defines a structure for language, the application of grammar is indescribably large in scope. Handling the variation in how language is used by humans is a major challenge for NLP systems. That's where machine learning can have a large impact. You can apply NLP to a range of problems. Some of the more common applications include search applications, such as Google or Bing, human-machine interactions, like Alexa, sentiment analysis for marketing or political campaigns, social research based on media analysis, and chatbots to mimic human speech in applications. You can apply the machine learning development pipeline you've seen throughout this course when developing an NLP solution. The first task is to formulate a problem, then collect and label data. For NLP, collecting data consists of breaking down the text into meaningful subsets and labeling the sets. Feature engineering is a major part of NLP applications. This process gets complicated when you're dealing with highly irregular or unstructured text. For example, say you're building an application to classify documents. You'd need to be able to distinguish between the words with common terms, but different meanings. Labeling data in the NLP domain is sometimes also called tagging. In the labeling process, you assign individual text strings to different parts of speech. There are specialized tools you can use for NLP labeling. The first task for an NLP application is to convert the text to data so it can be analyzed. You convert text by removing words that aren't needed for analysis from the input text. In the example, the words this and is are removed to leave the phrase sample text. After removing stop words, you can normalize text by converting similar words into a common form. For example, the words run, runner, ran, and running are all different forms of the word run. You can normalize all instances of these words in a block of text using the stemming and limitization processes. Limitization groups different forms of a word into a single term. Limitization of the versions of the word run would group all instances of those forms into a single term, run. Stemming, on the other hand, removes characters that the stemming algorithm considers unnecessary. Stemming might not work with the run example as the form ran might not be recognized as a form of the word run. After you've normalized the text, you can standardize it by removing words that aren't in the dictionary you're using for analysis. For example, you could remove acronyms, slang, and special characters. The Natural Language Toolkit is also known as NLTK. Their Python library provides functions for removing stop words and normalizing text. Another first step in creating an NLP system is to convert the text into a data collection, such as a data frame. All NLP libraries provide functions to assist with this process. The example shows using the word tokenize function from the NLTK library. After you've cleaned up your text and loaded it into a data frame, you can apply one of the NLP models to create features. Here are a couple of common models. The first model is known as bag of words. This is a simple model for capturing the frequency of words in a document. The model creates a key for each word. The value of the key is the number of times that word occurs in the document. The second model is term frequency and inverse document frequency, which is also known as TFIDF. Term frequency is a count of how many times a word appears in a document. Inverse document frequency is the number of times a word occurs in a group of documents. These two values are used together to calculate a weight for the words. Words that frequently appear in many documents have a lower weight. There are many established models in the NLP field. The example shows a bag of words model. Bag of words is a vector model. Vector models convert each sentence or phrase into a vector, which is a mathematical object that records both directionality and magnitude. In the example, a simple sentence is converted into a vector, where the frequency of each word is recorded. The word is has a value of two because it appears twice in the sentence. Bag of words is often used to classify documents into different categories. It's also used to derive attributes that feed into NLP applications, such as in sentiment analysis. There are three broad categories of text analysis. First, the classification of text is similar to other classification systems you've seen in this course. Text provides the input to a process that extracts features. Then you send the features through a machine learning algorithm that interacts with a classifier model and infers the classification. There are many applications for text matching. For example, autocorrect spelling and grammar checking are based on text matching. The algorithm for edit distance, also known as the Levingstein distance, is frequently used. You can derive relationships between different words or phrases in the text using a process called co-reference resolution. Several NLP systems provide Python libraries for deriving relationships. One of the biggest challenges for NLP is how to describe the context for the text. Consider this example, where a user is searching for the term tablet. Because the word tablet has at least two distinct meanings, the search engine needs to know which meaning the user has in mind. Most search engines rely on the most commonly used context if the term isn't qualified further. For example, by adding another term like medicine or computing to the search. The process of extracting entities is known as named entity recognition or NER. An NER model has the following functions. First, it can identify noun phrases using dependency charts and part of speech tagging. It can classify phrases using a classification algorithm such as word to VEC. Finally, it can disambiguate entities using a knowledge graph. Here's an example of using NER to extract the entity's Titanic and North Atlantic from the text. After the named entities are extracted, you can use a knowledge graph to extract meaning. A knowledge graph combines subject matter expertise with machine learning to derive meaning. The Amazon Recommendations Engine is an example of a knowledge graph. Here are the main points to remember from this section. First, NLP predates machine learning. You can use the same ML workflow that you've seen in other modules for NLP. Some of the main use cases for NLP are search query analysis, human-machine interaction, and marketing and social research. NLP is complicated because human language lacks precision. Thanks for watching. We'll see you in the next video.