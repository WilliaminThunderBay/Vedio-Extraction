 Hi, welcome back. We're now going to look at a few ways you can collect and secure data. In this section, we'll explore some of the techniques and challenges associated with collecting and securing the data that's needed for machine learning. Consider again the original example about predicting credit card fraud. You've further formulated the problem. But what data do you need to actually train your model so you can get the desired output and subsequently achieve your intended business outcome? Do you have access to the data? If so, how much data do you have and where is it? What solution can you use to bring all this data into one centralized repository? The answers to these questions are essential at this stage. The good news for a budding data scientist is that there are many places where you can obtain data. Private data from you or your existing customer already exists, including everything from log files to customer invoice databases. Private data can be useful depending on the problem you're trying to solve. In many cases, private data is found in many different systems. We'll look at how to bring these sources together shortly. Sometimes you want to use data that was collected and made available by a commercial organization. Many such as Reuters, Change Healthcare, Dunham Bradstreet and Foursquare maintain databases you can subscribe to. They include curated news stories, anonymized healthcare transactions, global business records, and location data. If you supplement your own data with commercial data, you can get useful insights you wouldn't have gotten otherwise. There are also many open source data sets, ranging from wine quality to movie reviews. These data sets are made available for use in research or for teaching purposes. AWS, Kaggle, and the UCI Machine Learning Repositories are good places to find open source data sets. Governments and health organizations are other sources of data that could be useful. AWS Machine Learning problems need a lot of data. These are also called observations, and you already need to know the target answer or prediction for that data. This kind of data, where you already know the target answer or prediction, is called labeled data. Each observation in your data is made up of two elements, the target and the features. The target is the answer you want to predict. So in the credit card transaction example, the target of any given observation is either fraud or not fraud. A feature is an attribute of the example that you can use to identify patterns for predicting the target answer. A feature in the credit card example could be the date of the transaction, the vendor, or the amount in dollars of the transaction. You might wonder if the source of the target is fraud or not fraud. Typically, this information is discovered only after the transaction is complete, and the actual card owner notices a fraudulent transaction on their statement. This information would be recorded with the transaction for exactly the purpose of using it to train a future model. So given what you know about the elements of an ML dataset, we'll return to one of the original questions. What data do you need to actually train your model to reach the desired output, and subsequently your intended business outcome? This is an example of a stage in the ML pipeline when it's crucial to get domain expertise to help you answer this question. With domain knowledge, you can start determining the features and target data your model will need to make accurate predictions. Your data should be representative of the data you'll have when you're using the model to make a prediction. For example, if you want to predict credit card fraud, you need to collect data for positive or fraudulent transactions. You also need to collect data for negative or non-fraudulent transactions. You need both types of data, so the machine learning algorithm can find patterns that will distinguish between the two types. Suppose your average amount of fraudulent transactions is actually 3%, but your training dataset only includes a very small fraction of fraudulent observations, say 0.4%. In this case, it'll be difficult for your model to truly learn patterns related to fraudulent transactions that it might encounter in production. There are many different services in AWS where you could find or store your data. Here are some key services you might use. Amazon Simple Storage Service is also known as Amazon S3. It provides object-level storage. With S3, you can store as much data as you want in the form of objects, which you can think of as files. They could be CSV files or files of other formats you need. S3 can be accessed through the web-based AWS Management Console. You can also access S3 programmatically through the API and SDKs or with third-party solutions, which also use the API and SDKs. If your training data is already in S3 and you're planning to run training jobs several times with different algorithms and parameters, you could use Amazon FSX for Lustre. It's a file system service that speeds up your training jobs by serving your S3 data to Amazon SageMaker at high speeds. The first time you run a training job, FSX for Lustre automatically copies data from S3 and makes it available to SageMaker. You can use the same Amazon FSX file system for subsequent iterations of training jobs, which prevents repeated downloads of common S3 objects. Alternatively, your training data might already be in Amazon Elastic File System or Amazon EFS. If so, we recommend using EFS as your data source for training data. It can launch your training jobs directly from the service without needing data movement, which results in faster training start times. This is often the case in environments where data scientists have home directories in Amazon EFS. They can quickly iterate on their models by bringing in new data, sharing data with colleagues, and experimenting with different fields or labels in their data set. For example, a data scientist can use a Jupyter notebook to do an initial cleansing on a training set and launch a training job from Amazon SageMaker. They could then use their Jupyter notebook to drop a column and relaunch the training job and finally compare the resulting models to see which one works better. There are many other AWS services and resources where you might find data. For example, you could use Amazon Relational Database Service or Amazon RDS, a managed relational database service. You could also use Amazon Redshift, which is a managed data warehouse service. Another option is Amazon Timestream, a managed time series database designed specifically to handle large amounts of data from the Internet of Things or IoT. You could even spin up your own instances on Amazon Elastic Compute Cloud, which is also known as Amazon EC2, and host your own database on these instances. When you have data sources, you'll need to extract useful data from these sources when assembling your data for machine learning. We'll look at this next. That's it for part one of this section. We'll see you again for part two, where we'll review how to extract, transform, and load data.