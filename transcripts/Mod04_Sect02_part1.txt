 Hi, and welcome back. This is section 2, and we're going to focus on processing time series data, because it can be different from other types of data you've been using so far. Time series data is data that is captured in chronological sequence over a defined period of time. Introducing time into a machine learning model has a positive impact, because the model can derive meaning from changes in the data points over time. Time series data tends to be correlated. This means that there's a dependency between data points. This has mixed results for forecasting. This is because you're dealing with a regression problem, and regression assumes that data points are independent. You need to develop a method for dealing with data dependence, so you can increase the validity of the predictions. In addition to the time series data, you can add related data to augment a forecasting model. For example, suppose you want to make a prediction about retail sales. You could include information about the product being sold, such as item identification, or sales price, along with the number of units sold per time period. The third type of data is metadata about the dataset. For instance, say that you have a retail dataset. You might want to include metadata, like a brand name or a genre for music or videos, so you can group results. It's better to have more data. When you work with multiple data sources, you'll face the challenge of handling the time stamp of the data. You'll observe differences in the time stamp format and other challenges, such as incomplete data. However, you might be able to infer missing data in some cases. For example, say you have some data that contains both the month and the day, but no year. Observe whether the data seems to sequence through the month numbers in the database, repeating after 12. If it does so, you could add the year if you knew when the data started. You could then infer future years based on the order of the data. Such time stamp data is stored in UTC format, but not all data is. You should check if the time stamp is in local or universal time. Sometimes, the time stamp doesn't represent the time you think it does. For example, suppose you have a database of cars that were serviced at a garage. Does the time stamp indicate the time the car arrived, was completed, or picked up? Or does it indicate when the final entry was entered into the system? Say you're trying to model the hourly caloric intake of patients. However, you only have daily data. Then you'll need to adjust your target time scale. Also, your data might not have any time stamps. There could be other ways to extrapolate a time series depending on the data and domain. For example, you might have wavelength measurements or vectors within an image. As a final note, remember that daylight savings is different around the world. Also, because of daylight savings time, might even occur twice a year in their time zones. A common occurrence in real-world forecasting problems is missing values in the raw data. Missing values makes it harder for a model to generate a forecast. The primary example in retail is an out-of-stock situation in demand forecasting. If an item goes out of stock, the sales for the day will zero. If the forecast is generated based on those zero sales values, the forecast will be incorrect. There are many reasons why values can be marked as missing. Missing values can occur because of no transaction. They can also occur because of possible measurement errors. For example, a service that monitored certain data wasn't working correctly. Or as another example, the measurement couldn't happen correctly. In retail, the primary example for an inability to take correct measurements is an out-of-stock situation in demand forecasting. This means that demand doesn't equal sales on that day. There are several ways you can calculate the missing data. The first method is forward fill. This uses the last known value for the missing value. Depending on that idea, moving average uses the average of the last known values to calculate the missing value. Backward fill uses the next known value after the missing value. The danger here is that you're using the future to calculate the past, which is bad in forecasting. This method is also known as look ahead and should be avoided. Interpolation uses an equation to calculate the missing value. You can also use a zero fill. This is often used in retail because missing sales data shouldn't be calculated. The missing data represents that there were no orders on that day. It would be wise to investigate why this happened, but in this case, you don't want to fill in the missing value. You might get data at different frequencies. For example, you might have sales data that includes the exact time stamp the sale was recorded, but have inventory data that only contains the year, month, and day of the inventory level. When you have data that's at a different frequency than other data sets, or data that's not compatible with your question, you might need to down sample. Down sampling is moving from a more finely grained time to a less finely grained time. As the example shows, this could be converting an hourly data set to a daily data set. When down sampling, you need to decide how to combine the values. In the previous case of sales data, summing the quantity makes the most sense. If the data is temperature, you might want to find the average. Understanding your data helps you decide what's the best course of action. The opposite of down sampling is up sampling. When you move from a less finely grained time to a more finely grained time. The problem with up sampling is that it's extremely difficult to achieve in most cases. Suppose you want to up sample your sales data from daily sales to hourly sales. Unless you have some other data source to reference, you wouldn't be able to do this. There are cases when you need to do something, perhaps to match the frequency of another time series. Or you might have an irregular time series, or specific domain knowledge that would help. In those cases, you need to be careful how you make the conversion. For the retail example, the best you could do is create a single order for the day at a specified hour. For temperature, you could copy the daily temperature into each hourly slot, or use a formula to calculate a curve. In data science, outliers have a mix of positive and negative attributes. The same is true of time series data. Suppose you were examining sales data, and you had an order that has an unusually high number of items. You might not want to include that in your forecast calculations, because the order size might never be repeated. Removing these outliers in anomalies is known as smoothing. Smoothing your data can help you deal with outliers in other anomalies. There are a few reasons why you might consider smoothing. First, during data preparation, you remove error values and could also remove outliers. You might also want to smooth your data to generate features. For visualization, you could smooth your data to reduce the noise in a plot. It's important to understand why you are smoothing the data and the impact that it might have. The outcome might be to reduce noise and create a better model. But an equally important question is, could your smoothing compromise the model? Is the model expecting noisy data? Will you also be able to smooth the data in production? That's it for part one of this section. We'll see you again for part two, where we'll review more time series specific challenges and the tools and algorithms that can help us wrangle your data.