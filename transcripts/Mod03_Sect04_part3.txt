 Hi, welcome back. We'll continue exploring feature engineering by describing how to work with outliers. You might also need to clean your data based on any outliers that exist. Outliers are points in your dataset that lie at an abnormal distance from other values. They're not always something you want to clean up because they can add richness to your dataset. But they can also make it harder to make accurate predictions because they skew values away from the other, more normal values related to that feature. An outlier might also indicate that the data point actually belongs to another column. You can think of outliers as falling into two broad categories. The first is a single variation for just a single variable, or a univariate outlier. The second is a variation of two or more variables, or a multivariate outlier. One of the more common ways to find univariate outliers is with a box plot. A box plot shows how far a data point is to the mean for that variable. The box in the plot shows the data values within two quartiles of the mean. Values outside that range are represented by the lines extending from the box, which are sometimes called whiskers. A scatter plot can be an effective way to see multivariate outliers. For example, this diagram shows the amount of sulfates and alcohol in a collection of wines. With the scatter plot, you can quickly visualize whether there are multivariate outliers for the two variables. The origin of your outlier will most likely inform how you deal with it during this pre-processing phase of the pipeline, or possibly later during feature engineering. There are several different approaches to dealing with outliers. You could delete the outlier if your outlier is based on an artificial error. This means the outlier isn't natural and was introduced because of some failure, like incorrectly entered data. You could also transform the outlier by taking the natural log of a value. This in turn reduces the variation caused by the extreme outlier value, which would then reduce the outlier's influence on the overall data set. Finally, you could use the mean of the feature and impute that value to replace the outlier value. Again, this would be a good approach if the outlier was caused by artificial error. This isn't an exhaustive list, but it describes the most common options. After you've extracted features, you'll need to select the most appropriate features for training your model. There are three main feature selection methods. Filter methods use statistical methods to measure the relevance of features by their correlation with the target variable. Wrapper methods measure how useful a subset of a feature is. They do this by training a model on the feature and then measuring how successful the model is. Models are faster and cheaper than wrapper methods because they don't involve training the models repeatedly. Wrappers typically find the best subset of features, but there's a risk of overfitting compared to using subsets of features from filter methods. Embedded methods are algorithm specific, and they might use a combination of both filters and wrappers. Filter methods use a proxy measure instead of the actual model's performance. They're fast to compute, but they can still capture how useful the feature set is. Here are some common measures. The first is Pearson's correlation coefficient, which measures the statistical relationship or association between two continuous variables. The second is linear discriminant analysis, or LDA. This is used to find a linear combination of features that separates two or more classes. The third is analysis of variance, or ANOVA. This is used to analyze the differences among group means in a sample. And finally, chi-square is a single number that tells you how much difference exists between your observed counts and the counts you'd expect if there were absolutely no relationships in the population. Rappers are usually less computationally intensive than wrappers, but they produce a feature set that isn't tuned to a specific type of predictive model. This lack of tuning means a feature set from a filter is more general than one from a wrapper. The filter also usually has a lower prediction performance than a wrapper. However, the filter's feature set doesn't contain the assumptions of a prediction model, so it's more useful for exposing relationships between features. Many filters provide feature ranking instead of an explicit best feature subset, and the cutoff point in the ranking is chosen through cross-validation. Filters have also been used as a pre-processing step for wrappers, which enables a wrapper to be used on larger problems. Wrapper methods use a predictive model to score feature subsets. Each new subset is used to train a model, which is then tested on a holdout set. The score for that subset is calculated by counting the number of mistakes made on that holdout set, or the error rate of the model. Because wrappers train a new model for each subset, they're computationally intensive. However, they usually provide the best performing feature set for that particular type of model or problem. Backward selection starts with no features and adds them until the best model is found. Backward selection starts with all features, drops them one at a time, and then selects the best model. Embedded methods combine the qualities of filter and wrapper methods. They're implemented by algorithms that have their own built-in feature selection methods. Some of the most popular examples of these methods are lasso and ridge regression. They have built-in penalization functions to reduce overfitting. Here are some key takeaways from this section of the module. First, feature engineering involves selecting the best features for machine learning. Pre-processing gives you better data to work with and better data typically provides better results. Two categories for pre-processing are converting data to numerical values and cleaning up dirty data by removing missing data and cleaning outliers. Finally, how you handle dirty data impacts your model. That's it for section 4. We'll see you in the next video.