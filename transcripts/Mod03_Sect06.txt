 Hi, and welcome back. This is section 6, and we're going to look at hosting and using the model. In this section, we'll look at how you can deploy your trained model so it can be consumed by applications. After you've trained, tuned, and tested your model, you'll learn more about testing in the next section, you're now ready to deploy your model. If you're thinking that we're looking at the phases out of order, here's why we're discussing deployment now. If you want to test your model and get performance metrics from it, you first need to make an inference or prediction from the model, and this typically requires deployment. Deployment for testing is different from production, although the mechanics are the same. Amazon SageMaker provides everything you need to host your model for simple testing and evaluation, from a few requests to deployments handling tens of thousands of requests. There are two ways you can deploy your model. For single predictions, you can deploy your model with Amazon SageMaker hosting services. SageMaker will deploy multiple compute instances which run your model behind a load balanced end point. Applications can call the API at the end point to make predictions. With this model, you can scale the number of instances up or down based on demand. To get predictions for an entire dataset, use Amazon SageMaker batch transform. Instead of deploying and maintaining a permanent end point, SageMaker will spin up your model and perform the predictions for the entire dataset you provide. It will then store the results in Amazon S3 before it shuts down and terminates the compute instances. It's useful for performing batch predictions when you test the model. You can quickly run your entire validation set against the model without writing any code to process and collate the individual results. The goal of the deployment phase is to provide a managed environment to host models for providing inference securely and with low latency. After your model is deployed into production, you should monitor your production data and retrain your model if necessary. Newly deployed models need to reflect the current production data. New data is accumulated over time and it could potentially identify alternative or new outcomes. And so deploying a model is not a one-time exercise. Instead it's a continuous process. With one click, you can deploy your model on Amazon ML instances that can automatically scale across multiple availability zones for higher redundancy. Just specify the type of instance and the maximum and minimum number of instances desired. SageMaker will take care of the rest. It will launch the instances, deploy your model, and set up the secure HTTPS endpoint for your application. Your application only needs to include an API call to this endpoint to achieve inference with low latency and high throughput. With this architecture, you can integrate your new models into your application in minutes because changes to the model no longer need changes to the application code. SageMaker manages your production compute infrastructure on your behalf. It can perform health checks, apply security patches, and conduct other routine maintenance, all with built-in Amazon CloudWatch monitoring and logging. After you've trained the model, you can create the endpoint either in code or by using the SageMaker console. If you're planning to host only a single model, you can create an endpoint for that model. But if you're planning to host multiple models, you need to create a multi-model endpoint. Multi-model endpoints provide a scalable and cost-effective solution for deploying large numbers of models. They use a shared serving container that's enabled to host multiple models. This reduces hosting costs by improving endpoint utilization, compared to using single model endpoints. It also reduces deployment overhead because SageMaker manages loading models in memory and scaling the models based on the traffic patterns to them. When you deploy machine learning models into production to make predictions on new data, you need to make sure you apply the same data processing steps that were used in training to each inference request. Otherwise, you can get incorrect prediction results. By using inference pipelines, you can reuse the data processing steps from model training during inference without maintaining two separate copies of the same code. This helps ensure the accuracy of your predictions and reduces development overhead. Because SageMaker is a managed service, inference pipelines are completely managed. When you deploy the pipeline model, the service installs and runs the sequence of containers on each EC2 instance in the endpoint or each batch transform job. Additionally, the sequence of feature processing and inference runs with low latency because the containers are collated on the same EC2 instances. Some key takeaways from this section of the module include these points. You can deploy your trained model by using SageMaker to handle API calls from applications or to perform predictions using a batch transformation. The goal of your model is to generate predictions to answer the business problem. Be sure that your model can generate good results before you deploy to production. Finally, use multi-model endpoint support to save resources when you have multiple models to deploy. That's it for this section. We'll see you in the next video.