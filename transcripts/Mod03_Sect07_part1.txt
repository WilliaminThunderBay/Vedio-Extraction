 Hi, welcome back to Module 3. In this section, we'll look at how you can evaluate your model's success in predicting results. At this point, you've trained your models. It's now time to evaluate that model to determine if it will do a good job predicting the target on new and future data. Because future instances have unknown target values, you need to assess how the model will perform on data where you already know the target answer. You'll then use this assessment as a proxy for performance on future data. This is the reason why you hold out a sample of your data for evaluating or testing. An important part of this phase involves choosing the most appropriate metric for your business situation. Think back to the earlier section on problem formulation. During that phase, you define your business problem and outcome, and then you craft a business metric to evaluate success. The model metric you choose at this phase should be linked to that business metric as much as possible. There's often a high correlation between the two metrics. In addition to considering your business problem and success metric, the type of ML problem you're working with will influence the model metric you choose. Throughout the rest of this module, we'll look at examples of common metrics used in classification problems. We'll also look at common metrics used in regression problems. We're going to start by considering a simple binary classification problem. Here's a specific example. Imagine that you have a simple image recognition model that's labeling data as either cat or not cat. After the model's been trained, you can use the test data set you held back to perform predictions. To help examine the performance of the model, you can compare the predicted values with the actual values. If you plot the values into a table like the example, you can start getting some insights into how well the model performed. In a confusion matrix, you can get a high-level comparison of how the predicted class is matched up against the actual classes. If the actual label or class is cat, which is identified as P for positive, and the predicted label or class is also cat, then you have a true positive. This is a good outcome for your model. Similarly, if you have an actual label of not cat, which is identified as N for negative, and the predicted label or class is also not cat, then you have a true negative. This is also a good outcome for your model. In both these cases, your model predicted the correct outcome when it used the testing data. There are two other possible outcomes, and both aren't considered good outcomes. The first one is when the actual class is negative, so you got not cat, but the predicted class is positive, or cat. This is called a false positive, because the prediction is positive, but incorrect. Finally, there are false negatives. These happen when the actual class is positive, so you got cat, but the predicted class is negative, or not cat. That's it for part one of this section. We'll see you again for part two, where we'll review calculating classification metrics.