 Hi, welcome back. We'll continue exploring how to evaluate your model. Classification models are going to return a probability for the target. This is a value of the input belonging to the target class, and it will be between 0 and 1. To convert the value to a class, you need to determine the threshold to use. You might think it's 50%, but you could change it to be lower or higher to improve your results. As you've seen with sensitivity and specificity, there's a trade-off between correctly and incorrectly identifying classes. Changing the threshold can impact that outcome. We're going to take a look at how you can visualize this. A receiver operating characteristic graph is also known as an ROC graph. It summarizes all the confusion matrices that each threshold produced. To build one, you calculate and plot the sensitivity, or true positive rate, against the false positive rate on a graph for each threshold value. You can calculate the false positive rate by subtracting the specificity from 1. After you plot those points, you can draw a line between them. The dotted black line from 0,0 to 1,1 means that the sensitivity, or true positive rate, is equal to the false positive rate. The point at 1,1 means that you've correctly identified all the cats, but you've also incorrectly identified all the not-cats. This is bad. Any point on this line means that the proportion of correctly classified samples is the same as the proportion of incorrectly classified samples. The point at 0,0 represents that there are zero true positives and zero false positives. A model that has high sensitivity and low false positive rate is usually the goal, so it's considered to be better when the line between the threshold recordings is closer towards the top left corner. If you had the data from two models, you could plot out the ROC curve for each model and compare them. However, that can be tedious. There's another graph you can use for this which we'll look at next. Another evaluation metric you can use is the area under the curve receiver operator curve, which is also known as an AUC ROC. The AUC part is the area under the plotted line. When the AUC is higher, it means the model will be better at predicting cats as cats and not-cats as not-cats. You can use the AUC to quickly compare models with each other. With the four numbers from our confusion matrix, you can calculate the model's accuracy. This is also known as its score. You can do this by adding up the correct predictions and then dividing that number by the total number of predictions. Though accuracy is widely used metric for classification problems, it has limitations. This metric isn't as effective when there are a lot of true negative cases in your data set. Think about the cat not cat example. If most of your accuracy is based on true negatives, it says that your model is good at predicting what isn't a cat. In this case, you might not feel confident in your model's ability to predict cats after you roll it out into production. This leads to an example of why it's important to make sure that the metric you choose for model evaluation aligns to your business goal. Think about the credit card fraud example. In this case, using accuracy as your main metric probably isn't a good idea because you have a lot of true negatives. Your high true negative number might hide the fact that your model's ability to identify cases of fraud, that is to identify true positives, isn't ideal. As a credit card company, it's probably unacceptable to have less than almost perfect performance identifying fraud cases. That would drive customers away, which would be the opposite of what you'd want to achieve from a business standpoint. This is why two other metrics are often used in these situations. The first one is precision, which essentially removes the negative predictions. Precision is the proportion of positive predictions that are actually correct. You can calculate it by taking the true positive and dividing it by true positive plus false positive. When the cost of false positives is high in your particular business situation, precision might be a good metric. Think about a classification model that identifies email messages as spam or not. In this case, you don't want your model to label an email message as spam and thus prevent your users from seeing that message when it's actually legitimate. Or consider an example of a model that needs to predict whether a patient has a terminal illness. In this case, using precision as your evaluation metric doesn't account for false negatives in your model. Consider for the model to be successful, it's crucial that it doesn't falsely predict the absence of illness in a patient who actually has that illness. Sensitivity would be a better metric to use for this situation. But it doesn't always need to be one or the other. The F1 score combines precision and sensitivity together. It gives you one number that quantifies the overall performance of a particular ML algorithm. You should consider using an F1 score when you have a class imbalance but want to preserve the equality between precision and sensitivity. But what do you do if you're dealing with a regression problem? In that case, there are other common metrics you can use to evaluate your model, including the mean squared error. The mean squared error is frequently used. Its general purpose is the same as what you saw with classification metrics. You determine the prediction from the model and you compare the difference between the prediction and the actual outcome. More specifically, you take the difference between the prediction and actual value, square that difference, and then sum up all the squared differences for all the observations. In Skykit Learn, you can use the mean squared error function directly from the metrics library. There are other metrics you can use for linear models, such as R squared. So you've trained your model, performed a batch transformation on your test data, and calculated your metrics. Now what will you do? You'll use these metrics to help you tune the model. You could select a different set of features and train the model again. After you retrain the model, ask yourself, which was the better model? The metrics will help inform you. You could also use different data and retrain the model with the same features. Remember K-fold cross-validation from earlier in this module? Finally, you could tune the parameters of the model itself, which is the subject of the next section. Here are key takeaways from this section of the module. To evaluate the model, you need to have data that the model hasn't seen. This could be either a holdout set, or you could use K-fold cross-validation. Different machine learning models use different metrics. Classification can use the confusion matrix and the AUC-ROC that you can generate from it. Regression can use mean squared. That's it for section 7. See you in the next video.