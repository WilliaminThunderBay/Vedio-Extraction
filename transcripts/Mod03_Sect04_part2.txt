 Hi, welcome back. We'll continue exploring feature engineering by reviewing how to clean your dataset. In addition to converting string data to numerical data, you'll need to clean your dataset for several other potential problem areas. Before encoding the string data, make sure the strings are all consistent. You'll also need to make sure variables use a consistent scale. For example, if one variable describes the number of doors in a car, the scale will probably be between two and eight. But if another variable describes the number of cars of a particular type sold in the state of California, the scale will probably be in the thousands. Some data items might also capture more than one variable in a single value. For instance, suppose the dataset includes variables that combine safety and maintenance into a single variable, such as safe high maintenance. You'll need to train your machine learning system for both variables, and also split that single variable into two separate variables. You might also encounter datasets that are missing data for some variables, and some datasets will include outliers. We'll cover techniques for dealing with these situations in this section. You might find that data is missing. For example, some columns in your dataset could be missing data because of a data collection error, or maybe data wasn't collected on a particular feature until the data collection process was underway. Missing data can make it difficult to accurately interpret the relationship between the related feature and the target variable. So regardless of how the data ended up being missed, it's important for you to deal with this issue. Unfortunately, most machine learning algorithms can't handle missing values automatically. You'll need to use human intelligence to update missing values with data that's meaningful and relevant to the problem. Most Python libraries for data manipulation include functions for finding missing data. So how do you decide if you should drop or impute missing values? This question is answered in part by better understanding how those values came to be missing in the first place, and how much data the missing values represent within your larger dataset. For instance, say the missing values are randomly spread throughout your dataset and don't represent a larger portion of its respective row or column. In this case, imputation is most likely the better option. In contrast, say that you have a column or row that has a large percentage of missing values. In this case, dropping the entire row or column would be preferred over imputation. If you decide to drop rows with missing data, you can use built-in functions to do this. For example, Pandas DropNA function can drop all rows with missing data, or you can drop specific data values by using a subset. As an alternative to dropping missing values, you can impute values for those missing values. There are different ways to impute a missing value. For categorical values, the missing value is usually replaced with the mean, the median, or the most frequent values. For numerical or continuous variables, the missing value is usually replaced with the mean or the median. You can impute a single row of missing data, which is known as univariate. You can also do this for multiple rows, which is known as multivariate. We'll now look at a univariate example. Here, the scikit-learn imputer function is being used to impute some missing values. It's a fairly small dataset, but there are two missing values. The missing value was imputed by the strategy of the mean. To do this, you first calculate the mean. Here, it's the mean of 3 and 2, which is 2.5. Then you'll impute the mean value for the missing value. Some data libraries include an impute package that provides more complex ways to impute data. Examples include K nearest neighbor, soft impute, multiple imputation by chain of equations, and others. That's it for part two of this section. We'll see you again for part three, where we'll review how to work with outliers in your data.