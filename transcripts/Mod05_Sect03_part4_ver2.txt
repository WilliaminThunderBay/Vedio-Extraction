 Hi, welcome back. We'll continue exploring video analysis by reviewing how to evaluate and improve your model. In general, you can improve the quality of your model with larger quantities of better quality data. Use training images that clearly show the object or scene, and don't include many things that you're not interested in. For bounding boxes around objects, use training images that show the object as fully visible and not hidden by other objects. Make sure that your training and test data sets match the type of images that you'll eventually run inference on. For objects where you have just a few training examples, like logos, you should provide bounding boxes around the logo in your test images. These images represent the scenarios you want to localize the object in. Reducing false positives often results in better precision. To reduce false positives, first, check if increasing the confidence threshold enables you to keep the correct predictions while eliminating false positives. Increasing the confidence threshold eventually results in diminishing gains because of the tradeoff between precision and recall for a given model. Next, check to see if you need to add additional classes for training. For example, if you are detecting cats, but often dogs are being flagged as cats, add dog as a label to your training data set along with the images of dogs that you got the false positive on. Effectively, you're helping the model learn to predict dog and not cat through the new training images. You might find that the model is confused between two of your custom labels, cat and dog. The test image with label cat is predicted as having labeled dog and vice versa. In this case, first, check for mislabeled images in your training and test sets. Also, adding more training images that reflect this confusion will help a retrained model learn to better discriminate between cat and dog. Reducing false negatives often results in better recall. To reduce false negatives, first, lower the confidence threshold. This should improve recall. Also, use better examples to model the variety of both the object and the images they appear in. Finally, split your label into two classes that are easier to learn. For example, instead of good cookies and bad cookies, you might want good cookies, burnt cookies and broken cookies to help the model learn each unique concept better. If you're satisfied with the performance of your model, you can make it available for use by starting it from the console or by using code. After the model is running, you can perform an inference with the AWS CLI or the SDK. When you call the API, you specify the Amazon resource name of the Amazon Recognition Custom Labels model that you want to use. The Amazon resource name is also known as an ARN. You'll also specify the image you want the model to make a prediction with. You can provide an input image as an image byte array of base 64 encoded image bytes or as an S3 object. Custom labels are returned in an array of custom label objects. Each custom label represents a single object, scene or concept that's found in the image. A custom label includes a label for the object, scene or concept that was found in the image. It also includes a bounding box for objects that were found in the image. The bounding box coordinates show where the object is located on the source image. The coordinate values are a ratio of the overall image size. Finally, the custom label includes the confidence score. This represents how confident Amazon Recognition Custom Labels is in the accuracy of the label and bounding box. During training, a model calculates a threshold value that determines if a prediction for a label is true. By default, the detect custom labels operation doesn't return labels with a confidence value that's less than the model's calculated threshold value. To filter the returned labels, specify a value for min confidence that's greater than the model's calculated threshold. You can get the model's calculated threshold from the model's training results in the Amazon Recognition Custom Labels console. To get all the labels regardless of confidence, specify a min confidence value of zero. If you find that the confidence values returned by the detect custom labels operation are too low, consider retraining the model. You can restrict the number of custom labels that are returned from the detect custom labels operation by specifying the max results input parameter. The returned results are sorted from the highest confidence to the lowest confidence. Here are some key takeaways from this section of the module. Models must be trained for the specific domain that you want to analyze. If you're looking for turbochargers, you'll need many pictures of turbochargers to train your model. You can set custom labeling for the specific business case. We looked at the custom labeling process and some of the tools you can use. If you want objects to be detected, you need to label images and create bounding boxes for these objects. You can use Amazon SageMaker Ground Truth to build training datasets for your models, which can also use machine learning to label your images. Thanks for watching and we'll see you in the next video.