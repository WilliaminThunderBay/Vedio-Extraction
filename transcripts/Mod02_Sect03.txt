 Hi, and welcome back. This is section 3, and we're going to give you a quick, high-level overview of machine learning terminology and a typical workflow. We will cover these topics in more detail later in this course, but for now, we'll focus on the larger picture. So to begin, you should always start with the business problem you or your team believe could benefit from machine learning. From there, you want to do some problem formulation. In this phase, one task is to articulate your business problem and convert it to an ML problem. After you've formulated the problem, you move on to the data preparation and pre-processing phase. You'll pull data from one or more data sources. These data sources might have differences in data, or types that need to be reconciled so you can form a single, cohesive view of your data. You'll need to visualize your data and use statistics to determine if the data is consistent and can be used for machine learning. We'll look at some of the data sources later in the course. This example data has four columns containing data from three different data sources. The sources had slightly different ways of representing data, and the results are shown in the table. In ML problems, columns represent features, and rows represent instances. There are some issues here with the data in some of the instances. In some cases, you'll need a subject matter expert or a functional expert to understand the authenticity of the data. For example, the date that's represented as 11 to 1969 could be November 2 or February 11 in the year 1969. Anyone who owns or manages the data pool would be able to clarify this ambiguity. Also, the word mail can probably be attributed to an import issue where cells shifted position, but there could be an outside chance where it's the actual location, Malay, a city that's the capital of the Republic of Maldives. At times, this error identification isn't as simple, and you'll need an SME to review the data. You'll learn about the role of experts later in this course. Remember that one of the largest impacts you can have on the success of a machine learning project is to have consistent and correct data. After your data is in good shape, it's time to train your model. This is where the process gets very iterative and fluid. You'll likely go through many multiple passes of feature engineering, training, evaluating, and tuning before you find a model that meets your business goals. Feature engineering is the process of selecting or creating the features your model will be trained with. Features are the columns of data you have in your dataset. The goal of the model is to correctly estimate the target value for the new data. The ML algorithm will use the features to predict the target. In this example, the target data is the average number of steps taken in a week. Selecting the correct features can involve adding, removing, or calculating new features. You might want to make the data formats consistent. The consistent formats could be later used in the model, or you can make these changes for cosmetic reasons. Depending on the problem you want to solve with this data, you might not even need to include the name feature in the example data. What about the country feature? If this were a traditional database, you might want to move country to a lookup table, then reference it. Most ML algorithms want the data for an instance in a single row. ML algorithms need numerical data to process. You could consider turning the country text into the country's ISO code. However, the model might interpret the numerical value as having meaning. So the UK's ISO code value of 44 would be more significant than the ISO code value of the US, which is 01. In this case, splitting the data into multiple columns is fine. This is known as categorical encoding, and you'll learn about this later in the course. For other types of data, you could convert the text value into a numerical value. For example, you could use 0 or 1 to represent male or female. These numeric values can be used more easily by the model. What about the remaining features, like age, birth month, which is shown as B-M in the table, or day of week, which is shown as D-O-W? Extracting the age, birth month, and day of week might be appropriate depending on the problem you're trying to solve. Does age impact the target variable? What about which day of the week they were born on? Don't worry if this sounds complicated. You'll learn more about feature engineering later in this course. After your data is cleaned and you've identified the features you want to use, it's time to train a model. You won't use all the data to train your model. In fact, you need to hold some of the data so you have some data to test with. Typically, you'll use about 80% of the data to train with, and you'll save the rest of the data for testing. Next, you'll train a model with training data. In the diagram, the model uses the XG Boost algorithm. The model itself has some parameters you can set. These parameters will alter how the algorithm works and they're known as hyperparameters. The output of the training job will be a trained model. With the trained model, you can use some of the test data to see how well the model performs. You'll take an instance the model hasn't seen and use it to perform a prediction. Because you already know the target in your test data, you can compare the two values. From these comparisons, you can calculate metrics which give you data on how well the model is performing. You'll then make changes to the model's data, features or hyperparameters, until you find the model that yields the best results. When training your model, there's a real danger of overfitting or underfitting the model. Your model is overfitting your training data when you see the model performs well on the training data but doesn't perform well on the evaluation data. This is because the model is memorizing the data it saw and can't generalize to unseen examples. Your model is underfitting the training data when the model performs poorly on the training data. This is because the model can't capture the relationship between the input examples, which are often called X, and the target values, which are often called Y. Understanding model fit is important for understanding the root cause of poor model accuracy. This understanding will guide you to take corrective steps. You can determine whether a predictive model is underfitting or overfitting the training data by looking at the prediction error on the training data and the evaluation data. We'll show you steps you can take to avoid this later in this course. After you've retrained the model and you're satisfied with the results, you deploy your model to deliver the best possible predictions. Later in this course, we'll walk you through these different phases and give you hands-on experience with each of them. But knowing the process is also useful when using the managed services we'll also explore later where certain Amazon ML services will do the bulk of the work for you. Here are some of the key takeaways for this section. First, we looked at how the machine learning pipeline process can guide you through the process of training and evaluating a model. The iterative process can be broken into three broad steps, data processing, model training, and model evaluation. That's it for this video. We'll see you in the next one.