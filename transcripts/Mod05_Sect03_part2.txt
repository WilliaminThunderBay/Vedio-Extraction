 Hi, welcome back. We'll continue exploring video analysis by reviewing how to create the training dataset. Data sets contain information that's needed to train and test an Amazon Recognition Custom Labels model, such as images, labels, and bounding boxes. You can use images from Amazon S3, or you can upload them from your computer to S3 as part of the process. To train a model, your dataset should have at least two labels, with at least 10 images per label. Each image in your dataset must be labeled. As we mentioned earlier, you can use the Amazon Recognition Custom Labels Console or Amazon SageMaker Ground Truth to label your images. Again, to train an Amazon Recognition Custom Labels model, your images must be labeled. A label indicates that an image contains an object, scene, or concept. As we mentioned earlier, a dataset needs at least two defined labels. Also, each image must have at least one assigned label that identifies the object, scene, or concept in the image. When you apply labels to an image as a whole, these labels are known as image-level labels. They're useful for identifying scenes or concepts that you want to detect. For example, one of the images shows a beach scene from Ko'olina. It's on the island of Oahu in the U.S. state of Hawaii. To train a model to detect beaches, you'd add a beach label that applies to the entire image. You can also apply labels to specific areas of an image that contain an object you want to detect. For example, if you want your model to detect Amazon Echo devices, it must identify the different types of Echo devices in an image. The model needs information about where the devices are located in the image. And it needs a corresponding label that identifies the type of the device. This information is known as localization information. The location of the device is expressed as a bounding box. The example objects with bounding boxes image shows a bounding box that surrounds an Amazon Echo dot. The image also contains an Amazon Echo without a bounding box. The output of the labeling process will be a manifest file. The manifest file for an image-level label typically contains the label, or class name, along with some metadata about how the image was labeled. For object detection, the manifest contains information about each labeled image. The bounding box identifies where the object is in the image, along with the label that the bounding box belongs to. We've mentioned Amazon SageMaker Ground Truth a few times. We'll now look at what it is and how it might help you. With SageMaker Ground Truth, you can build high-quality training datasets for your machine learning models. To use it, create a dataset that needs labeling. You then provide detailed instructions on what needs to be labeled and submit the job. You can decide who processes the images to create a labeled dataset. You can use workers from the Amazon Mechanical Turk service, a vendor company, or an internal workforce with machine learning. You can use the labeled dataset output from SageMaker Ground Truth to train your own models, or you can also use it with Amazon Recognition Custom Labels. SageMaker Ground Truth can use active learning to automate the labeling of your input data. Active learning is a machine learning technique that identifies data that should be labeled by your workers. In SageMaker Ground Truth, this functionality is called automated data labeling. Automated data labeling can reduce the time and cost it takes to label your dataset, compared to using only human workers. When you use automated labeling, you incur Amazon SageMaker training and inference costs. Yes, we just said that you can use machine learning to label the images that you'll then use for machine learning. We'll talk through how this works. When SageMaker Ground Truth starts an automated data labeling job, it selects a random sample of input data, or objects, and sends it to human workers. When the labeled data is returned, SageMaker Ground Truth uses this data, which is the validation data, to validate the models that were trained for automated data labeling. SageMaker Ground Truth runs a batch transform job using the validated model for inference on the validation data. Batch inference produces a confidence score and quality metric for each object in the validation data. Automatic automated labeling determines if the confidence score for each object, which was produced in step 5, meets the required threshold, which was determined in step 4. If the confidence score meets the threshold, the expected quality of automatic labeling exceeds the requested level of accuracy. The object is then considered to be automatically labeled. Step 6 produces a dataset of unlabeled data with confidence scores. SageMaker Ground Truth selects data points with low confidence scores from this dataset and sends them to human workers for additional labeling. SageMaker Ground Truth then uses the existing human labeled data and the additional human labeled data to train a new model. The process is repeated until the dataset is fully labeled, or until another stopping condition is met. For example, automatic labeling can stop when you meet your budget for human annotation. We recommend using automated data labeling on large datasets. The minimum number of objects allowed for automated data labeling is 1,250. However, we strongly suggest providing a minimum of 5,000 objects. That's it for part 2 of this section. We'll see you again for part 3 where we'll review how to evaluate and improve your model.