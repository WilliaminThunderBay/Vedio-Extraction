 Hi, and welcome back to Module 3. This is Section 8. In this section, we're going to take a look at how you can tune the model's hyperparameters to improve model performance. Recall from an earlier module that hyperparameters can be thought of as the knobs that tune the machine learning algorithm to improve its performance. Now that we're looking more explicitly at tuning models, it's time to look more specifically at the different types of hyperparameters and how to perform hyperparameter optimization. There are a couple of different categories of hyperparameters. The first kind are model hyperparameters. They help define the model itself. As an example, consider a neural network for a computer vision problem. For this case, additional attributes of the architecture need to be defined, like filter size, pooling, and the stride or padding. The second kind are optimizer hyperparameters. They relate to how the model learns patterns based on data, and they're used for a neural network model. These types of hyperparameters include optimizers like gradient descent and stochastic gradient descent. They can also include optimizers that use momentum, like atom, or that initialize the parameter weights with methods like Xavier initialization or he initialization. The third kind are data hyperparameters. They relate to the attributes of the data itself. These include attributes that define different data augmentation techniques, like cropping or resizing for image-related problems. They're often used when you don't have enough data or enough variation in your data. Tuning hyperparameters can be very labor-intensive. Traditionally, this was done manually by someone who had domain experience related to the hyperparameter and the use case. This person would manually select the hyperparameters based on their intuition and experience. Then they would train the model and score it on the validation data. This process would be repeated over and over again until they achieved satisfactory results. This manual process isn't always the most thorough and efficient way of tuning hyperparameters. With SageMaker, you can perform automated hyperparameter tuning with Amazon SageMaker automatic model tuning. It finds the best version of a model by running multiple training jobs on your dataset by using the algorithm and hyperparameter ranges you specify. It then chooses the hyperparameter values that results in a model that performs the best as measured by a metric you choose. It uses Gaussian process regression to predict which hyperparameter values might be most effective at improving fit. It also uses Bayesian optimization to balance exploring the hyperparameter space and exploiting specific hyperparameter values when appropriate. And importantly, automatic model tuning can be used with built-in algorithms from SageMaker, pre-built deep learning frameworks, and bring your own algorithm containers. Suppose that you want to solve a binary classification problem on a fraud dataset. Your goal is to maximize the area under the AUC curve metric of the algorithm by training a linear learner algorithm model. You don't know which values of the learning rate, beta 1, beta 2, and epochs you should use to train the best model. To find the best values for these hyperparameters, you can specify ranges of values that SageMaker hyperparameter tuning will then search. It will find the combination of values that results in the training job that performs the best as measured by the objective metric that you chose. In the example, SageMaker hyperparameter tuning launches training jobs that use hyperparameter values in the ranges you specified and then returns the training job with the highest AUC. Hyperparameter tuning might not necessarily improve your model. It's an advanced tool for building machine solutions. As such, it should be considered part of the scientific method process. When you build complex machine learning systems, like deep learning neural networks, exploring all possible combinations is impractical. To improve optimization, use the following guidelines when you create hyperparameters. First, instead of using all hyperparameters, limit the number of hyperparameters to the ones you think would give you good results. The range of values for the hyperparameters you choose to search can significantly affect the success of hyperparameter optimization. Although you might want to specify a large range that covers every possible value for a hyperparameter, you'll get better results by limiting your search to a small range of values. If you get the best metric values within a part of a range, consider limiting the range to only that part. During hyperparameter tuning, SageMaker attempts to figure out if your hyperparameters are log-scaled or linear-scaled. Initially, it assumes that hyperparameters are linear-scaled. If they should be log-scaled, it might take time for SageMaker to discover that on its own. If you know that a hyperparameter should be log-scaled and you can convert it yourself, doing so can improve hyperparameter optimization. Running more hyperparameter tuning jobs concurrently gets more work done quickly, but a tuning job improves only through successive rounds of experiments. Typically, running one training job at a time achieves the best results with the least amount of compute time. Say that you have a distributed training job that runs on multiple instances. In this case, hyperparameter tuning uses the last reported objective metric from all instances of that training job as the value of the objective metric for that training job. Design distributed training jobs so that they report the objective metric you want. Now that you've gone through the end-to-end process of training and tuning a machine learning model, it's worth talking about Amazon SageMaker Autopilot. This service can help you find a good model with little effort or input on your part. With Autopilot, you create a job that supplies the test, training, and target. Autopilot will analyze the data, select appropriate features, and then train and tune the models. It will document the metrics and find the best model based on the provided data. The results include the winning model and metrics and a Jupyter notebook you can use to investigate the results. Although using Autopilot doesn't remove your need to pre-process the data, it can save you time during feature selection and model tuning. Some key takeaways from this section of the module include these points. First, model tuning is important for finding the best solution to your business problem. Actors can be tuned for the model, optimizer, and data. SageMaker can perform automatic hyperparameter tuning. And finally, overall model development can be accelerated by using Autopilot. That's it for this video. See you in the next one.